{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9855d2b-0fee-40fa-aa35-273691f31197",
   "metadata": {},
   "source": [
    "# 강화학습과 지도학습의 비교\n",
    "\n",
    "|| 지도학습(신경망) | 강화 학습 |\n",
    "| :-- | :-- | :--|\n",
    "| 문제(데이터) | 훈련 집합 X(특징 벡터)와 Y(레이블) | 환경 또는 환경에서 수집한 데이터(에피소드) |\n",
    "| 최적화 목표 | 신경망 출력 o와 레이블 y의 오차인 abs(o-y)의 최소화 | 누적 보상 최대화 |\n",
    "| 학습 알고리즘이 알아내야 하는 것 | 오차를 최소화하는 신경망의 가중치 | 누적 보상을 최대화하는 최적 정책 |\n",
    "| 품질을 평가하는 함수 | 손실 함수 | 가치 함수 |\n",
    "| 학습 알고리즘 | 스토케스틱 그레디언트 하강법(SGD) | 동적 프로그래밍, Sarsa, Q러닝, DQN 등 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bc8dab-91f0-44a6-92bb-f8d0ce234655",
   "metadata": {},
   "source": [
    "- 데이터\n",
    "    - 지도학습은 훈련 집합을 데이터로 활용\n",
    " \n",
    "    - 강화 학습은 에피소드가 데이터\n",
    " \n",
    "- 최적화 목표\n",
    "    - 지도 학습은 신경망 출력과 정답의 차이를 최소화\n",
    " \n",
    "    - 강화 학습은 누적 보상을 최대화\n",
    "        - 다중 슬롯머신 문제에서는 에피소드 시작부터 끝까지 발생한 보상의 총액(순수익)을 최대화\n",
    "        - 바둑은 작은 집을 희생하더라도 최종 집 수를 더 많게 해 승리하는 것이 목표\n",
    "     \n",
    "- 알고리즘이 알아내야 하는 대상\n",
    "    - 지도학습은 오차를 최소화하는 신경망의 가중치\n",
    "    - 강화 학습은 누적 보상을 최대화하는 최적 정책"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de3d675-7dfb-4d21-8ec0-63798b2cd207",
   "metadata": {},
   "source": [
    "# 최적 정책\n",
    "\n",
    "- 강화학습은 누적 보상을 최대화하는 최적 정책을 알아내야 함\n",
    "\n",
    "- 최적 정책\n",
    "    - 슬롯머신 문제에서는 손잡이3의 확률이 가장 높기 때문에 항상 손잡이3을 당기게 됨\n",
    " \n",
    "    - FrozenLake 에서는 칸 4에 있을 때 아래로 이동하면 항상 안전하게 얼음 위를 이동할 수 있음\n",
    " \n",
    "    - 정책은 확률 분포로 표현할 수 있음\n",
    "        - 현재 상태에서 어떤 행동을 취할지를 결정하는 확률 분포\n",
    "        - P(a=i|s=j)\n",
    "            - 상태변수 s가 j라는 값일 때 행동 변수 a가 i라는 값을 취할 확률\n",
    "            - 간결하게 P(i|j) 로 표기함\n",
    "         \n",
    "    - 슬롯머신의 최적 정책\n",
    "        - 승률이 최대인 손잡이만 당겨야 누적 보상을 최대화 할 수 있음\n",
    "            - 최대 승률인 손잡이의 확률을 1로 설정한 확률 분포가 최적\n",
    "         \n",
    "        - 최적 정책 : P(a=0|.) = 0, P(a=1|.) = 0, P(a=2|.) = 0, P(a=3|.) = 1, P(a=4|.) = 0\n",
    "     \n",
    "    - FrozenLake 의 최적 정책\n",
    "        - P(0|0) = 0, P(1|0) = 0.5, P(2|0) = 0.5, P(3|0) = 0\n",
    "            - 아래로 가는 확률과 오른쪽으로 가는 확률을 0.5씩 배분했기 때문에 오른쪽으로 가는 에피소드와 아래로 가는 에피소드가 반반씩 생성\n",
    "         \n",
    "            - 무조건 아래로만 내려가는 정책도 누적 보상을 최대로 한다는 목표로 보면 최적 정책이라고 할 수 있음\n",
    "         \n",
    "        - P(0|1) = 0, P(1|1) = 0, P(2|1) = 1, P(3|1) = 0\n",
    "            - 항상 오른쪽으로 가는 정책\n",
    "         \n",
    "        - ...\n",
    "     \n",
    "        - P(0|14) = 0, P(1|14) = 0, P(2|14) = 1, P(3|14) = 0\n",
    "            - 14번 칸에서는 항상 오른쪽으로 이동해 목표 지점에 도달"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2b9999-97bd-4699-a534-e1249a885c19",
   "metadata": {},
   "source": [
    "# 가치 함수로 찾는 최적 정책\n",
    "\n",
    "- FrozenLake 문제는 단순하므로 최적 정책을 쉽게 찾을 수 있음\n",
    "\n",
    "- 실용적인 문제에서는 적절한 학습 알고리즘을 쓰지 않고서는 최적 정책을 알아낼 수 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae0a663-b34b-4b4f-9789-18287eca0637",
   "metadata": {},
   "source": [
    "## 가치 함수\n",
    "\n",
    "- 강화 학습의 알고리즘은 신경망의 학습 알고리즘과 비슷함\n",
    "    - 처음에는 어떤 정책이 좋은지 모르므로 랜덤하게 정책을 설정하고 출발\n",
    " \n",
    "    - 점차 개선하는 사이클을 반복하여 최적 정책에 접근\n",
    "        - 지도 학습에서는 손실 함수로 신경망 가중치의 품질을 평가\n",
    "     \n",
    "        - 강화 학습에서는 가치 함수(value function)를 통해 정책의 품질을 평가\n",
    "     \n",
    "        - Vπ(s) : 상태 s에서 정책 π가 선택할 행동이 얼마나 많은 보상을 제공할지를 평가\n",
    "     \n",
    "        - 학습 알고리즘이 할 일은 Vπ(s) 를 최대화하는 최적 정책을 찾는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c09ff4-85c0-4307-a02f-22195afe3cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6f1b84-8278-40ef-8822-b08675e1dd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0410c72-02fc-4cde-becb-f228cdd136c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7357b3e-b100-4ea2-a138-0bf516eb41bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c308b710-c5d2-4254-871e-1f1248364d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93495d07-c7a8-4656-a114-58c94d981363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05412cf-0663-478b-8319-48fa88666ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c951df-f98d-4f12-b4be-aa6fe0bd4880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc192c3a-b05e-46a5-a7e3-e6024f63b7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e78cdc-e1ca-47f8-b1e8-660758ecfb73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb288ef-9e82-47d9-9ce2-269238770dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b7928-d524-48bd-ae40-e51022bd5214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9c35da-78bb-41ac-bdf5-6be52be830a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d15d12b-5b3c-421d-8152-aee244fa00c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374eaa2b-05bb-4b65-9379-4eeeacc52167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c708fffd-5317-41ef-89d2-cb03b53a9e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a18c3b9-870b-4f68-8129-0a4549496346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852a1157-2cd0-4b38-8847-e985facdd97d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e17494-0d1d-42fb-8e39-801f2824c58d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e8e021-885a-4218-bf53-f84ca82feddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6af0e0-7ed9-4b6e-b09e-4d7dbc3ac022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb51d1d-8fcd-4488-9fd6-32e0fe2c5227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc5cb11-8086-4e79-8e51-cd904ea85a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a824bc1-db47-4aab-96fa-a712562336b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c64104-4416-47a6-9ca5-03051c2368e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbebffcb-69aa-49e5-9f31-18158c5d8f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
